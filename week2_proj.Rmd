---
title: "week2_proj"
author: "uzma"
date: "28 November 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/Bruker/Desktop/datascience/Final_project/final/en_US")
```

## Coursera Data Science Capstone (Week 2)

The goal of this capstone is to mimic the experience of being a data scientist by using data science techniques learned from all 9 specialization courses to create a data product and presentation to Swiftkey.

For Week 2, the main objective is to build the sample corpus, find the 2-gram and 3-gram term document matrix and perform exploratory analysis on the words. The data is available to be downloaded from

https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

The files are extracted from the zip file with three working files:

"en_US.blogs.txt"
"en_US.news.txt"
"en_US.twitter.txt"

## Data Preparation
Several library chosen to begin with are as below:


```{r libraries}
library(NLP)
library(tm)
library(stringi)
library(RWeka)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
options(mc.cores=1)
```

## Data Import & Manipulation

The large data files are read into RStudio with the readLines functionality.




```{r Import}
setwd("C:/Users/Bruker/Desktop/datascience/Final_project/final/en_US")
lines_blog <- readLines(con <- file('en_US.blogs.txt'))
lines_twitter <- readLines(con <- file('en_US.twitter.txt'))
lines_news <- readLines(con <-  file('en_US.news.txt'))
length(lines_blog)
length(lines_twitter)
length(lines_news)
```

As these files are large, and computationally expensive to work with, the data was sampled to include just the first 100 entries from the original files. Given the large size of the data, the samples should be more than large enough to yield statistically significant results that pertain to the complete population, particularly at this stage of the project. After sampling, the data is aggregated into a single data set for analysis.

```{r sample}
setwd("C:/Users/Bruker/Desktop/datascience/Final_project/final/en_US")
sample_blog <- readLines(con <- file('en_US.blogs.txt'),100)
sample_twitter <- readLines(con <-  file('en_US.twitter.txt'),100)
sample_news <- readLines(con <- file('en_US.news.txt'),100)
```

The corpus will then be generated by using the sample created.

```{r corpus}
sData <- c(sample_blog, sample_twitter, sample_news)
dataCorpus <- VCorpus(VectorSource(sData))
```

##  Data Cleaning & Exploratory Analysis
To clean the sample data file, the following were removed: Punctuation, Extra Whitespaces, Stopwords, and Numbers. The file was also transformed to all lowercase.


```{r cleaning}
dataCorpus <- tm_map(dataCorpus, removePunctuation)
dataCorpus <- tm_map(dataCorpus, removeNumbers)
dataCorpus <- tm_map(dataCorpus, stripWhitespace)
dataCorpus <- tm_map(dataCorpus, removeWords, stopwords("english"))
dataCorpus <- tm_map(dataCorpus, content_transformer(tolower))

```

## Build Term Document Matrix

Now, the sample corpus is ready, it will then be tokenized using NGramTokenizer to three different categories: the unigram,bigram and trigram to further analyze the frequency of the words.

A Term Document Matrix was created to rank commonly appearing words, and provide tabular output of the top words.

## 1-Gram

1-gram is a contiguous sequence of single word from the corpus.

```{r matrix}
tdm = TermDocumentMatrix(dataCorpus)
wordMatrix = as.data.frame((as.matrix(  tdm )) ) 
v <- sort(rowSums(wordMatrix),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
plotd<-d[1:20,]
plotd
```

##2-Gram
2-gram is a contiguous sequence of two words from the corpus.
```{r 2gram}
bigram <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm2<-TermDocumentMatrix(dataCorpus,control = list(tokenize = bigram))
wordMatrix2 = as.data.frame((as.matrix(  tdm2 )) ) 
v2 <- sort(rowSums(wordMatrix2),decreasing=TRUE)
d2 <- data.frame(word = names(v2),freq=v2)
plotd2<-d2[1:20,]
plotd2
```

## 3-Gram
3-gram is a contiguous sequence of three words from the corpus.

```{r 3gram}

trigram <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tdm3<-TermDocumentMatrix(dataCorpus,control = list(tokenize = trigram))
wordMatrix3 = as.data.frame((as.matrix(  tdm3 )) ) 
v3 <- sort(rowSums(wordMatrix3),decreasing=TRUE)
d3 <- data.frame(word = names(v3),freq=v3)
plotd3<-d3[1:20,]
plotd3
```

## Generate Word Cloud and GGplot
Word Cloud and GGplot are generated to better illustrate the relationship of the words in each ngram categories. The top 100 words, 2-gram words and 3-gram words are shown on the word clouds and plot.

1-gram Word Cloud and Plot

```{r cloudplot}
wordcloud(dataCorpus, max.words=100, random.order=FALSE, colors=brewer.pal(8,"Dark2"))
wordcloud(plotd$word,plotd$freq, max.words=100, random.order=FALSE, colors=brewer.pal(8,"Dark2"))
wordcloud(plotd2$word,plotd2$freq, max.words=100, random.order=FALSE, colors=brewer.pal(8,"Dark2"))
wordcloud(plotd3$word,plotd3$freq, max.words=100, random.order=FALSE, colors=brewer.pal(8,"Dark2"))
```

Plot the most frequent  1 grams in a bar graph.

```{r 1barplot}
ggplot(head(plotd,15), aes(reorder(word,freq), freq)) +
  geom_bar(stat = "identity") + coord_flip() +
  xlab("1 grams") + ylab("Frequency") +
  ggtitle("Most frequent  1 grams")
```

The most frequent bigrams in a bar graph.
```{r 2barplot}
ggplot(head(plotd2,15), aes(reorder(word,freq), freq)) +
  geom_bar(stat = "identity") + coord_flip() +
  xlab(" bigrams") + ylab("Frequency") +
  ggtitle("Most frequent  bigrams")
```


The most frequent trigrams in a bar graph.
```{r 3barplot}
ggplot(head(plotd3,15), aes(reorder(word,freq), freq)) +
  geom_bar(stat = "identity") + coord_flip() +
  xlab("trigrams") + ylab("Frequency") +
  ggtitle("Most frequent   trigrams")
```

## Next Steps
the steps will be taken in order to generate a succesful working algorithm application.
- train and model data
- Build prediction model
- test the prediction model 
- and create a shiny app for that

